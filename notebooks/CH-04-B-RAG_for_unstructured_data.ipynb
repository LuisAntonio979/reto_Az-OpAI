{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reto 04-B - Generación Aumentada con Recuperación (RAG) para Datos No Estructurados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introducción\n",
    "\n",
    "Las empresas tienen mucha información propietaria que debe tenerse en cuenta al responder las preguntas de los usuarios; estas no siempre pueden ser respondidas a través de los datos con los que se han entrenado los modelos GPT.\n",
    "\n",
    "En el último notebook, trabajamos principalmente con datos estructurados. Muchas veces, los datos de tu empresa no se limitan solo a formatos estructurados como archivos CSV o tablas SQL. También pueden incluir datos no estructurados como documentos PDF o imágenes. De hecho, tus documentos individuales podrían tener tanto datos no estructurados como estructurados integrados. Extraer información de estos formatos diversos de una manera comprensible presenta un desafío. Herramientas como Azure Document Intelligence permiten la extracción de datos de fuentes no estructuradas como formularios o documentos. Una vez que los datos se extraen en un formato JSON estructurado, se puede utilizar AI Search para consolidar toda la información de diferentes tipos de datos en índices, facilitando la recuperación de documentos relevantes.\n",
    "\n",
    "En este notebook, te guiaremos a través de un caso de uso de Generación Aumentada con Recuperación (RAG) que implica trabajar con datos no estructurados. El enfoque RAG combina varias tecnologías para mejorar la calidad y la relevancia de las salidas generadas. Aprovecharemos Azure Document Intelligence para procesar documentos complejos, utilizando la API de layout para extraer texto y tablas de manera efectiva. Utilizaremos Azure AI Search para crear un índice configurando capacidades de búsqueda semántica, lo que permite la recuperación de páginas de documentos relevantes. Además, se incorporarán embeddings para recuperar contenido que esté lo más alineado posible con la pregunta del usuario. Finalmente, el modelo ChatGPT de Azure OpenAI utilizará el contenido extraído para generar una respuesta más significativa. Es importante enfatizar que este proceso de grounding (fundamentación) sigue el patrón RAG mencionado en el cuaderno anterior y ayuda a eliminar inexactitudes en las respuestas generadas.\n",
    "\n",
    "Tus objetivos para este desafío son leer este notebook, ejecutar cada bloque de código, observar los resultados y luego poder responder las preguntas planteadas en la guía del estudiante.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken==0.5.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (0.5.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tiktoken==0.5.2) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/codespace/.local/lib/python3.12/site-packages (from tiktoken==0.5.2) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken==0.5.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken==0.5.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken==0.5.2) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken==0.5.2) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "! pip install \"tiktoken==0.5.2\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Azure Forms Recognizer, Azure Cognitive Search, OpenAI, and other python modules\n",
    "\n",
    "\n",
    "import os, json, requests, sys, re\n",
    "import requests\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient \n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    SemanticConfiguration,\n",
    "    PrioritizedFields,\n",
    "    SemanticField,\n",
    "    SemanticSettings\n",
    ")\n",
    "\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "import openai\n",
    "import numpy as np\n",
    "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is secure and recommended way to load OpenAI resource credentials and deployment names\n",
    "\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "openai.api_base = os.environ['OPENAI_API_BASE']\n",
    "openai.api_type = os.environ['OPENAI_API_TYPE']\n",
    "openai.api_version = os.environ['OPENAI_API_VERSION']\n",
    "\n",
    "chat_model = os.environ['CHAT_MODEL_NAME']\n",
    "embedding_model=os.environ['EMBEDDING_MODEL_NAME']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTA:** La ruta en la celda de código a continuación se refiere a la carpeta `/data/unstructured/raw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- raw data\n",
    "RAW_DATA_FOLDER= '/workspaces/reto_Az-OpAI/data/unstructured/raw'\n",
    "# -- extracted json file \n",
    "EXTRACTED_DATA_FOLDER = '/workspaces/reto_Az-OpAI/data/unstructured/extracted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "\n",
    "endpoint = os.environ[\"AZURE_FORM_RECOGNIZER_ENDPOINT\"]\n",
    "key = os.environ[\"AZURE_FORM_RECOGNIZER_KEY\"]\n",
    "\n",
    "document_analysis_client = DocumentAnalysisClient(endpoint=endpoint, credential=AzureKeyCredential(key))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queremos extraer los datos de nuestros datos no estructurados a un formato más legible para que el modelo los entienda. La herramienta Document Intelligence nos ayuda a hacerlo aprovechando los modelos de diseño preconstruidos. Aquí, trabajamos principalmente con archivos PDF, pero también podríamos tener formatos JPG y PNG que la herramienta Document Intelligence también admite.\n",
    "\n",
    "Para cada documento, queremos especificar la forma en que se extrae la información. Por ejemplo, en este caso de uso, cada documento tiene muchas páginas. Para hacer un seguimiento de las páginas, las almacenamos en el campo page_number. También queremos extraer el contenido de cada página y colocarlo en un campo page_context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_local_single_file(file_name: str):\n",
    "    not_completed = True\n",
    "    while not_completed:\n",
    "        with open(file_name, \"rb\") as f:\n",
    "            poller = document_analysis_client.begin_analyze_document(\n",
    "                \"prebuilt-layout\", document=f\n",
    "            )\n",
    "            not_completed=False\n",
    "    result = poller.result()\n",
    "    return get_page_content(file_name, result)\n",
    "\n",
    "def extract_files( folder_name: str, destination_folder_name: str):\n",
    "    os.makedirs(destination_folder_name, exist_ok=True)\n",
    "    for file in os.listdir(folder_name):\n",
    "        if file[-3:].upper() in ['PDF','JPG','PNG']:\n",
    "            print('Processing file:', file, end='')\n",
    "        \n",
    "            page_content = extract_local_single_file(os.path.join(folder_name, file))\n",
    "            output_file = os.path.join(destination_folder_name, file[:-3] +'json')\n",
    "            print(f'  write output to {output_file}')\n",
    "            with open(output_file, \"w\") as f:\n",
    "                f.write(json.dumps(page_content))\n",
    "\n",
    "\n",
    "def get_page_content(file_name:str, result):\n",
    "    page_content = []\n",
    "    for page in result.pages:\n",
    "        all_lines_content = []\n",
    "        for line_idx, line in enumerate(page.lines):\n",
    "            all_lines_content.append(' '.join([word.content for word in line.get_words()]))\n",
    "        page_content.append({'page_number':page.page_number, \n",
    "                                'page_content':' '.join(all_lines_content)})\n",
    "    return {'filename':file_name, 'content':page_content}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels.pdf"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  write output to /workspaces/reto_Az-OpAI/data/unstructured/extracted/Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels.json\n",
      "Processing file: Prefix-Tuning_Optimizing_Continuous_Prompts_for_Generation.pdf  write output to /workspaces/reto_Az-OpAI/data/unstructured/extracted/Prefix-Tuning_Optimizing_Continuous_Prompts_for_Generation.json\n",
      "Processing file: Power_of_Scale_for_Parameter-Efficient_Prompt_Tuning.pdf  write output to /workspaces/reto_Az-OpAI/data/unstructured/extracted/Power_of_Scale_for_Parameter-Efficient_Prompt_Tuning.json\n"
     ]
    }
   ],
   "source": [
    "extract_files(RAW_DATA_FOLDER, EXTRACTED_DATA_FOLDER)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Más Sobre Nuestros Datos\n",
    "\n",
    "En este tutorial, examinaremos varios artículos de investigación sobre temas de LLM en documentos PDF. Esto incluye temas como:\n",
    "\n",
    "- Autoprompting (prompting automático)\n",
    "- Chain of thought prompting (prompting de cadena de pensamiento)\n",
    "- precise zero shot dense retrival (recuperación densa precisa de cero disparos)\n",
    "- y más. \n",
    "\n",
    "Este conjunto de datos contiene varios formatos no estructurados, como texto, tablas, gráficos y fórmulas.\n",
    "\n",
    "## Descripción de los Datos\n",
    "\n",
    "El esquema relevante para nuestro trabajo de hoy consiste en:\n",
    "\n",
    "- document_id\n",
    "- document_name\n",
    "- file_path\n",
    "- page_number\n",
    "- page_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=[]\n",
    "for file in os.listdir(EXTRACTED_DATA_FOLDER):\n",
    "    with open(os.path.join(EXTRACTED_DATA_FOLDER, file)) as f:\n",
    "        page_content= json.loads(f.read())\n",
    "    documents.extend(\n",
    "        [\n",
    "            {\n",
    "                'document_id':page_content['filename'].split('\\\\')[-1].split('.')[0].replace(\"/\", \"x\") + '-' + str(page['page_number']),\n",
    "                'document_name':page_content['filename'].split('\\\\')[-1].replace(\"/\", \"_\"),\n",
    "                'file_path':page_content['filename'].replace(\"/\", \"_\"),              \n",
    "                'page_number':page['page_number'],\n",
    "                'page_text':page['page_content']\n",
    "            }\n",
    "            for page in page_content['content']\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document_id': 'xworkspacesxreto_Az-OpAIxdataxunstructuredxrawxPower_of_Scale_for_Parameter-Efficient_Prompt_Tuning-2',\n",
       " 'document_name': '_workspaces_reto_Az-OpAI_data_unstructured_raw_Power_of_Scale_for_Parameter-Efficient_Prompt_Tuning.pdf',\n",
       " 'file_path': '_workspaces_reto_Az-OpAI_data_unstructured_raw_Power_of_Scale_for_Parameter-Efficient_Prompt_Tuning.pdf',\n",
       " 'page_number': 2,\n",
       " 'page_text': 'Model Tuning Pre-trained Model (11B params) Prompt Tuning a1 a2 Task A Batch b1 Task B Batch c1 Task C c2 Batch Mixed-task Batch Task A Model (11B params) A a1 A C c1 b1 a2 B A B Task B Model (11B params) -- C c2 C Task Prompts (20K params each) Task C Model (11B params) ---- Pre-trained Model (11B params) Figure 2: Model tuning requires making a task- specific copy of the entire pre-trained model for each downstream task and inference must be performed in separate batches. Prompt tuning only requires stor- ing a small task-specific prompt for each task, and enables mixed-task inference using the original pre- trained model. With a T5 “XXL” model, each copy of the tuned model requires 11 billion parameters. By contrast, our tuned prompts would only require 20,480 parameters per task—a reduction of over five orders of magnitude—assuming a prompt length of 5 tokens. low fine-tuned T5-XXL (Raffel et al., 2020) (71.8 vs. 89.3) despite using 16 times more parameters. Several efforts to automate prompt design have been recently proposed. Shin et al. (2020) propose a search algorithm over the discrete space of words, guided by the downstream application training data. While this technique outperforms manual prompt design, there is still a gap relative to model tuning. Li and Liang (2021) propose “prefix tuning” and show strong results on generative tasks. This method freezes the model parameters and back- propagates the error during tuning to prefix ac- tivations prepended to each layer in the encoder stack, including the input layer. Hambardzumyan et al. (2021) simplify this recipe by restricting the trainable parameters to the input and output sub- networks of a masked language model, and show reasonable results on classifications tasks. In this paper, we propose prompt tuning as a further simplification for adapting language models. We freeze the entire pre-trained model and only al- low an additional k tunable tokens per downstream task to be prepended to the input text. This “soft prompt” is trained end-to-end and can condense the signal from a full labeled dataset, allowing our method to outperform few-shot prompts and close the quality gap with model tuning (Figure 1). At the same time, since a single pre-trained model is recycled for all downstream tasks, we retain the ef- ficient serving benefits of frozen models (Figure 2). While we developed our method concurrently with Li and Liang (2021) and Hambardzumyan et al. (2021), we are the first to show that prompt tuning alone (with no intermediate-layer prefixes or task-specific output layers) is sufficient to be com- petitive with model tuning. Through detailed ex- periments in sections 2–3, we demonstrate that lan- guage model capacity is a key ingredient for these approaches to succeed. As Figure 1 shows, prompt tuning becomes more competitive with scale. We compare with similar approaches in Sec- tion 4. Explicitly separating task-specific param- eters from the “generalist” parameters needed for general language-understanding has a range of ad- ditional benefits. We show in Section 5 that by capturing the task definition in the prompt while keeping the generalist parameters fixed, we are able to achieve better resilience to domain shifts. In Sec- tion 6, we show that “prompt ensembling”, learn- ing multiple prompts for the same task, can boost quality and is more efficient than classic model en- sembling. Finally, in Section 7, we investigate the interpretability of our learned soft prompts. In sum, our key contributions are: 1. Proposing prompt tuning and showing its com- petitiveness with model tuning in the regime of large language models. 2. Ablating many design choices, and showing quality and robustness improve with scale. 3. Showing prompt tuning outperforms model tuning on domain shift problems. 4. Proposing “prompt ensembling” and showing its effectiveness. 2 Prompt Tuning Following the “text-to-text” approach of T5 (Raffel et al., 2020), we cast all tasks as text generation. Instead of modeling classification as the probabil- ity of an output class given some input, Pr(y|X), where X is a series of tokens and y is a single class label, we now model it as conditional generation, where Y is a sequence of tokens that represent a class label. T5 models classification as Prθ(Y |X), parameterized by the weights, θ, of the transform- ers (Vaswani et al., 2017) that make up its encoder and decoder. Prompting is the approach of adding extra in- formation for the model to condition on during its generation of Y . Normally, prompting is done by prepending a series of tokens, P, to the in- put X, such that the model maximizes the likeli- hood of the correct Y , Prθ(Y |[P; X]), while keep-'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example of a single page of research paper file that will be indexed in Azure Cognitive Search\n",
    "documents[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta sección se centrará en AI Search y los siguientes temas:\n",
    "\n",
    "1. Crear un índice de cliente\n",
    "2. Definir los campos del índice con los atributos necesarios\n",
    "3. Crear una configuración semántica\n",
    "4. Cargar nuestro índice con las páginas de los documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azure.search.documents.indexes._search_index_client.SearchIndexClient at 0x7ae2614cd100>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an SDK client\n",
    "service_endpoint = os.getenv(\"AZURE_COGNITIVE_SEARCH_ENDPOINT\")   \n",
    "key = os.getenv(\"AZURE_COGNITIVE_SEARCH_KEY\")\n",
    "credential = AzureKeyCredential(key)\n",
    "\n",
    "index_name = os.getenv(\"AZURE_COGNITIVE_SEARCH_DOC_INDEX_NAME\")\n",
    "\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=service_endpoint, credential=credential)\n",
    "index_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " research-paper-index-labp created\n"
     ]
    }
   ],
   "source": [
    "fields = [\n",
    "    SimpleField(name=\"document_id\", type=SearchFieldDataType.String, key=True),\n",
    "    SimpleField(name=\"page_number\", type=SearchFieldDataType.Int64),\n",
    "    SimpleField(name=\"file_path\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"document_name\", type=SearchFieldDataType.String,\n",
    "                searchable=True, retrievable=True),\n",
    "    SearchableField(name=\"page_text\", type=SearchFieldDataType.String,\n",
    "                filterable=True, searchable=True, retrievable=True),\n",
    "]\n",
    "\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"my-semantic-config\",\n",
    "    prioritized_fields=PrioritizedFields(\n",
    "        title_field=SemanticField(field_name=\"document_id\"),\n",
    "        prioritized_keywords_fields=[SemanticField(field_name=\"document_name\")],\n",
    "        prioritized_content_fields=[SemanticField(field_name=\"page_text\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# Create the semantic settings with the configuration\n",
    "semantic_settings = SemanticSettings(configurations=[semantic_config])\n",
    "\n",
    "# Create the search index with the semantic settings\n",
    "index = SearchIndex(name=index_name, fields=fields, semantic_settings=semantic_settings)\n",
    "result = index_client.create_or_update_index(index)\n",
    "print(f' {result.name} created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 41 documents\n"
     ]
    }
   ],
   "source": [
    "search_client = SearchClient(endpoint=service_endpoint, index_name=index_name, credential=credential)\n",
    "result = search_client.upload_documents(documents)  \n",
    "print(f\"Uploaded {len(documents)} documents\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Aquí vemos a Azure AI Search en acción! Podemos recuperar los documentos más relevantes de todos con los que estamos trabajando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is prompt tuning?\"\n",
    "count = 10\n",
    "results = search_client.search(search_text=query, top=count, include_total_count=True)\n",
    "page_chunks = []\n",
    "citations = []\n",
    "for result in results:\n",
    "    page_chunks.append(result['page_text'])\n",
    "    citations.append(result['document_name'])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Train Eval Tuning Accuracy F1 QQP MRPC Model P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Table 24: Few-shot exemplars for full chain of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100 --- 1 -+- 5 90 20 x -x- 100 150 80 SuperGL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0 20 30 Prefix Length (XSUM) 21.0- 36.0 20.5 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>matches, the best individual prompt. 7 Interpr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>model that we can reuse for prompt tuning acro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0 20 30 Prefix Length (XSUM) 21.0- 36.0 20.5 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Table 25: Few-shot exemplars for full chain of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Autoregressive Model (e.g. GPT2) Summarization...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Model Tuning Pre-trained Model (11B params) Pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         page_chunks\n",
       "0  Train Eval Tuning Accuracy F1 QQP MRPC Model P...\n",
       "1  Table 24: Few-shot exemplars for full chain of...\n",
       "2  100 --- 1 -+- 5 90 20 x -x- 100 150 80 SuperGL...\n",
       "3  0 20 30 Prefix Length (XSUM) 21.0- 36.0 20.5 3...\n",
       "4  matches, the best individual prompt. 7 Interpr...\n",
       "5  model that we can reuse for prompt tuning acro...\n",
       "6  0 20 30 Prefix Length (XSUM) 21.0- 36.0 20.5 3...\n",
       "7  Table 25: Few-shot exemplars for full chain of...\n",
       "8  Autoregressive Model (e.g. GPT2) Summarization...\n",
       "9  Model Tuning Pre-trained Model (11B params) Pr..."
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_df = pd.DataFrame(page_chunks, columns = [\"page_chunks\"]) #datframe with document chunks\n",
    "embed_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que tengamos los documentos más relevantes, crearemos embeddings para todos los fragmentos de página. Esto nos ayudará a encontrar los documentos más similares a nuestra consulta de usuario dada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Rate Limits\n",
    "\n",
    "from openai.error import RateLimitError\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "def get_embedding(text: str, engine: str = \"text-embedding-ada-002\"):\n",
    "    count=0\n",
    "    while True:\n",
    "        try:\n",
    "            embedding = openai.Embedding().create(input=[text], engine=engine)[\"data\"][0][\"embedding\"]\n",
    "            break;\n",
    "        except RateLimitError:\n",
    "            count+=1\n",
    "            #print(f'RateLimitError Count: {count}')\n",
    "            sleep(2)            \n",
    "    return np.array(embedding).astype(np.float32)\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-35-turbo\"): \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an embedding vector for each chunk that will capture the semantic meaning and overall topic of that chunk\n",
    "embed_df['embedding'] = embed_df[\"page_chunks\"].apply(lambda page_text : get_embedding(page_text, engine = embedding_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_chunks</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Train Eval Tuning Accuracy F1 QQP MRPC Model P...</td>\n",
       "      <td>[-0.013437969, -0.004924473, 0.037531715, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Table 24: Few-shot exemplars for full chain of...</td>\n",
       "      <td>[0.013712603, 0.012631297, 0.030331643, -0.004...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100 --- 1 -+- 5 90 20 x -x- 100 150 80 SuperGL...</td>\n",
       "      <td>[-0.011915736, -0.0039880956, 0.017686337, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0 20 30 Prefix Length (XSUM) 21.0- 36.0 20.5 3...</td>\n",
       "      <td>[-0.0014705374, 0.0114136385, 0.022483494, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>matches, the best individual prompt. 7 Interpr...</td>\n",
       "      <td>[-0.011753416, -0.011802099, 0.018569006, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>model that we can reuse for prompt tuning acro...</td>\n",
       "      <td>[-0.021523915, -0.0022356352, 0.010881379, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0 20 30 Prefix Length (XSUM) 21.0- 36.0 20.5 3...</td>\n",
       "      <td>[-0.0014705374, 0.0114136385, 0.022483494, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Table 25: Few-shot exemplars for full chain of...</td>\n",
       "      <td>[-0.0047118575, 0.011542224, -0.004891167, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Autoregressive Model (e.g. GPT2) Summarization...</td>\n",
       "      <td>[-0.018188149, 0.015794247, 0.015863037, -0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Model Tuning Pre-trained Model (11B params) Pr...</td>\n",
       "      <td>[-0.022335693, -0.010823373, 0.015005287, 0.00...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         page_chunks  \\\n",
       "0  Train Eval Tuning Accuracy F1 QQP MRPC Model P...   \n",
       "1  Table 24: Few-shot exemplars for full chain of...   \n",
       "2  100 --- 1 -+- 5 90 20 x -x- 100 150 80 SuperGL...   \n",
       "3  0 20 30 Prefix Length (XSUM) 21.0- 36.0 20.5 3...   \n",
       "4  matches, the best individual prompt. 7 Interpr...   \n",
       "5  model that we can reuse for prompt tuning acro...   \n",
       "6  0 20 30 Prefix Length (XSUM) 21.0- 36.0 20.5 3...   \n",
       "7  Table 25: Few-shot exemplars for full chain of...   \n",
       "8  Autoregressive Model (e.g. GPT2) Summarization...   \n",
       "9  Model Tuning Pre-trained Model (11B params) Pr...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [-0.013437969, -0.004924473, 0.037531715, -0.0...  \n",
       "1  [0.013712603, 0.012631297, 0.030331643, -0.004...  \n",
       "2  [-0.011915736, -0.0039880956, 0.017686337, -0....  \n",
       "3  [-0.0014705374, 0.0114136385, 0.022483494, -0....  \n",
       "4  [-0.011753416, -0.011802099, 0.018569006, -0.0...  \n",
       "5  [-0.021523915, -0.0022356352, 0.010881379, -0....  \n",
       "6  [-0.0014705374, 0.0114136385, 0.022483494, -0....  \n",
       "7  [-0.0047118575, 0.011542224, -0.004891167, -0....  \n",
       "8  [-0.018188149, 0.015794247, 0.015863037, -0.01...  \n",
       "9  [-0.022335693, -0.010823373, 0.015005287, 0.00...  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_chunks</th>\n",
       "      <th>embedding</th>\n",
       "      <th>similarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model Tuning Pre-trained Model (11B params) Pr...</td>\n",
       "      <td>[-0.022335693, -0.010823373, 0.015005287, 0.00...</td>\n",
       "      <td>0.866974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>matches, the best individual prompt. 7 Interpr...</td>\n",
       "      <td>[-0.011753416, -0.011802099, 0.018569006, -0.0...</td>\n",
       "      <td>0.846832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Train Eval Tuning Accuracy F1 QQP MRPC Model P...</td>\n",
       "      <td>[-0.013437969, -0.004924473, 0.037531715, -0.0...</td>\n",
       "      <td>0.836760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         page_chunks  \\\n",
       "0  Model Tuning Pre-trained Model (11B params) Pr...   \n",
       "1  matches, the best individual prompt. 7 Interpr...   \n",
       "2  Train Eval Tuning Accuracy F1 QQP MRPC Model P...   \n",
       "\n",
       "                                           embedding  similarities  \n",
       "0  [-0.022335693, -0.010823373, 0.015005287, 0.00...      0.866974  \n",
       "1  [-0.011753416, -0.011802099, 0.018569006, -0.0...      0.846832  \n",
       "2  [-0.013437969, -0.004924473, 0.037531715, -0.0...      0.836760  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embedding = get_embedding(query, engine=embedding_model)\n",
    "embed_df[\"similarities\"] = embed_df['embedding'].apply(lambda page_embedding: cosine_similarity(page_embedding, query_embedding))\n",
    "\n",
    "top_results = (\n",
    "    embed_df.sort_values(\"similarities\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    "    .head(3)\n",
    ")\n",
    "top_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A continuación se proporciona una consulta del usuario y una lista de páginas extraídas de artículos de investigación separadas por triple comillas invertidas.  \n",
      "Tu tarea es extraer las piezas clave de información de esa lista en función de la consulta del usuario y redactarla como una respuesta completa en español. \n",
      "\n",
      "Consulta del usuario: ```What is prompt tuning?```\n",
      "Lista de paginas extraidas: ```['Model Tuning Pre-trained Model (11B params) Prompt Tuning a1 a2 Task A Batch b1 Task B Batch c1 Task C c2 Batch Mixed-task Batch Task A Model (11B params) A a1 A C c1 b1 a2 B A B Task B Model (11B params) -- C c2 C Task Prompts (20K params each) Task C Model (11B params) ---- Pre-trained Model (11B params) Figure 2: Model tuning requires making a task- specific copy of the entire pre-trained model for each downstream task and inference must be performed in separate batches. Prompt tuning only requires stor- ing a small task-specific prompt for each task, and enables mixed-task inference using the original pre- trained model. With a T5 “XXL” model, each copy of the tuned model requires 11 billion parameters. By contrast, our tuned prompts would only require 20,480 parameters per task—a reduction of over five orders of magnitude—assuming a prompt length of 5 tokens. low fine-tuned T5-XXL (Raffel et al., 2020) (71.8 vs. 89.3) despite using 16 times more parameters. Several efforts to automate prompt design have been recently proposed. Shin et al. (2020) propose a search algorithm over the discrete space of words, guided by the downstream application training data. While this technique outperforms manual prompt design, there is still a gap relative to model tuning. Li and Liang (2021) propose “prefix tuning” and show strong results on generative tasks. This method freezes the model parameters and back- propagates the error during tuning to prefix ac- tivations prepended to each layer in the encoder stack, including the input layer. Hambardzumyan et al. (2021) simplify this recipe by restricting the trainable parameters to the input and output sub- networks of a masked language model, and show reasonable results on classifications tasks. In this paper, we propose prompt tuning as a further simplification for adapting language models. We freeze the entire pre-trained model and only al- low an additional k tunable tokens per downstream task to be prepended to the input text. This “soft prompt” is trained end-to-end and can condense the signal from a full labeled dataset, allowing our method to outperform few-shot prompts and close the quality gap with model tuning (Figure 1). At the same time, since a single pre-trained model is recycled for all downstream tasks, we retain the ef- ficient serving benefits of frozen models (Figure 2). While we developed our method concurrently with Li and Liang (2021) and Hambardzumyan et al. (2021), we are the first to show that prompt tuning alone (with no intermediate-layer prefixes or task-specific output layers) is sufficient to be com- petitive with model tuning. Through detailed ex- periments in sections 2–3, we demonstrate that lan- guage model capacity is a key ingredient for these approaches to succeed. As Figure 1 shows, prompt tuning becomes more competitive with scale. We compare with similar approaches in Sec- tion 4. Explicitly separating task-specific param- eters from the “generalist” parameters needed for general language-understanding has a range of ad- ditional benefits. We show in Section 5 that by capturing the task definition in the prompt while keeping the generalist parameters fixed, we are able to achieve better resilience to domain shifts. In Sec- tion 6, we show that “prompt ensembling”, learn- ing multiple prompts for the same task, can boost quality and is more efficient than classic model en- sembling. Finally, in Section 7, we investigate the interpretability of our learned soft prompts. In sum, our key contributions are: 1. Proposing prompt tuning and showing its com- petitiveness with model tuning in the regime of large language models. 2. Ablating many design choices, and showing quality and robustness improve with scale. 3. Showing prompt tuning outperforms model tuning on domain shift problems. 4. Proposing “prompt ensembling” and showing its effectiveness. 2 Prompt Tuning Following the “text-to-text” approach of T5 (Raffel et al., 2020), we cast all tasks as text generation. Instead of modeling classification as the probabil- ity of an output class given some input, Pr(y|X), where X is a series of tokens and y is a single class label, we now model it as conditional generation, where Y is a sequence of tokens that represent a class label. T5 models classification as Prθ(Y |X), parameterized by the weights, θ, of the transform- ers (Vaswani et al., 2017) that make up its encoder and decoder. Prompting is the approach of adding extra in- formation for the model to condition on during its generation of Y . Normally, prompting is done by prepending a series of tokens, P, to the in- put X, such that the model maximizes the likeli- hood of the correct Y , Prθ(Y |[P; X]), while keep-', 'matches, the best individual prompt. 7 Interpretability An ideally interpretable prompt would consist of natural language that clearly describes the task at hand, explicitly asks the model for some result or action, and makes it easy to understand why the prompt elicited such behavior from the model. As prompt tuning works in the continuous em- bedding space rather than the discrete token space, interpreting prompts becomes more difficult. To test the interpretability of our learned soft prompts, we compute the nearest neighbors to each prompt token from the frozen model’s vocabulary. We use cosine distance between the vocabulary embedding vector and the prompt token representation as the similarity metric. We observe that for a given learned prompt to- ken, the top-5 nearest neighbors form tight seman- tic clusters. For example, we see lexically similar clusters such as { Technology / technology / Tech- nologies / technological / technologies }, as well as more diverse but still strongly related clusters such as { entirely / completely / totally / altogether / 100% }. The nature of these clusters suggests that the prompts are in fact learning “word-like” repre- sentations. We found that random vectors drawn from the embedding space do not show this sort of semantic clustering. When initializing the prompts using the “class- label” strategy, we often find that the class labels persist through training. Specifically, if a prompt token is initialized to a given label, that label is often among the learned token’s nearest neighbors after tuning. When initializing with the “Random Uniform” or “Sampled Vocab” methods, the class labels can also be found in the nearest neighbors of the prompts; however they tend to appear as neighbors to multiple prompt tokens. This suggests that the model is learning to store the expected output classes in the prompts as reference, and initializing the prompt to outputs classes makes this easier and more centralized. When examining longer prompts (e.g. size 100), we often find several prompt tokens with the same nearest neighbors. This suggests there is either excess capacity in the prompt, or that the lack of sequential structure in the prompt representation makes it difficult for the model to localize informa- tion to a specific position. While the learned prompts taken as sequences show little interpretability, we do observe a high frequency of words like science, technology and engineering as the nearest neighbors for prompts trained on the BoolQ dataset and approximately 20% of the questions are in the “Nature/Science” category. While more investigation is needed, this suggests that one role of the prompt may be to prime the model to interpret inputs in a specific domain or context (e.g. “scientific”). 8 Conclusion In this paper, we showed that prompt tuning is a competitive technique for adapting frozen pre- trained language models to downstream tasks. On the popular SuperGLUE benchmark, its task perfor- mance rivals that of traditional model tuning, with the gap vanishing as model size increases. On zero- shot domain transfer, we found that prompt tuning leads to improved generalization. This plausibly in- dicates that freezing general-purpose language un- derstanding parameters and restricting downstream learning to a lightweight parameter footprint can help to avoid overfitting to a specific domain. Beyond task quality metrics, we discussed the appeal of moving to frozen pre-trained models in terms of storage and serving costs. This move enables both efficient multi-task serving, as well as efficient high-performing prompt ensembling. Looking forward, we believe that factoring out task-defining parameters as distinct from general language-modeling parameters is an exciting step that opens up many avenues for new research. Acknowledgements We thank Lucas Dixon, Waleed Ammar, Slav Petrov and Sebastian Ruder for comments on an earlier draft, and the following people for helpful discussion: Colin Raffel, Adam Roberts, and Noam Shazeer. We thank Linting Xue for help with the LM adaptation training. References Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. 2006. The second PASCAL recognising textual entailment challenge. In Proceedings of the second PASCAL challenges workshop on recognis- ing textual entailment, volume 6, pages 6–4. Venice. Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. 2009. The fifth PASCAL recognizing textual entailment challenge. In TAC.', 'Train Eval Tuning Accuracy F1 QQP MRPC Model Prompt 73.1 ±0.9 76.3 ±0.1 81.2 ±2.1 84.3 ±0.3 MRPC QQP Model Prompt 74.9 ±1.3 75.4 ±0.8 70.9 ±1.2 69.7 ±0.3 Table 2: Mean and stddev of zero-shot domain transfer between two paraphrase detection tasks. cific lexical cues and spurious correlations. This re- striction suggests that prompt tuning may improve robustness to domain shifts, where the distribution of inputs differs between training and evaluation. We investigate zero-shot domain transfer on two tasks: question answering (QA) and paraphrase de- tection. For question answering, we use the MRQA 2019 shared task on generalization (Fisch et al., 2019). This task collects extractive QA datasets in a unified format and tests how models trained on “in-domain” datasets perform when evaluated on “out-of-domain” datasets. For our experiments, we train on SQuAD (Rajpurkar et al., 2016) and evaluate on each of the out-of-domain datasets.11 Table 1 shows that prompt tuning outperforms model tuning on the majority of out-of-domain datasets, with a remarkable 12.5 point F1 gap be- tween the two approaches on TextbookQA. We ob- serve larger gains from prompt tuning in cases of larger domain shifts (e.g. to Biomedical in BioASQ or to Textbooks in TextbookQA). Of the datasets where model tuning is better, we see that DROP shares a domain (Wikipedia) with SQuAD and is thus one of the smallest domain transfers. As a second test of robustness to domain shift, we explore transfer between two paraphrase detec- tion tasks from GLUE (Wang et al., 2019b). The first task is QQP (Iyer et al., 2017), which asks if two questions from the community Q&A site Quora are “duplicates”. The second task is MRPC (Dolan and Brockett, 2005), which asks if two sen- tences drawn from news articles are paraphrases. We test transfer in both directions (QQP ⇔ MRPC). As before, we train on the “in-domain” task, select checkpoints using in-domain validation, and evalu- ate zero-shot on the “out-of-domain” task. Table 2 shows that training a lightweight prompt on the QQP data and evaluating on MRPC gives much better performance than tuning the entire 11We select checkpoints based on SQuAD validation F1. The out-of-domain datasets are TextbookQA (Kembhavi et al., 2017), RACE (Lai et al., 2017), BioASQ (http://bioasq. org/), RE (Levy et al., 2017), DuoRC (Saha et al., 2018), and DROP (Dua et al., 2019). Dataset Metric Average Best Ensemble BoolQ acc. 91.1 91.3 91.7 CB acc./F1 99.3 / 99.0 100.00 / 100.00 100.0 / 100.0 COPA acc. 98.8 100.0 100.0 MultiRC EM/F1a 65.7 / 88.7 66.3 / 89.0 67.1 / 89.4 ReCoRD EM/F1 92.7 / 93.4 92.9 / 93.5 93.2 / 93.9 RTE acc. 92.6 93.5 93.5 WiC acc. 76.2 76.6 77.4 WSC acc. 95.8 96.2 96.2 SuperGLUE (dev) 90.5 91.0 91.3 Table 3: Performance of a five-prompt ensemble built from a single frozen T5-XXL model exceeds both the average and the best among the five prompts. model (+3.2 accuracy and +3.1 F1). The results are much closer in the other direction, with prompt tuning showing a small improvement in accuracy and a small drop in F1. These results support the view that model tuning may be over-parameterized and more prone to overfit the training task, to the detriment of similar tasks in different domains. 6 Prompt Ensembling Ensembles of neural models trained from different initializations on the same data are widely observed to improve task performance (Hansen and Salamon, 1990) and are useful for estimating model uncer- tainty (Lakshminarayanan et al., 2017). However, as model size increases, ensembling can become impractical. Beyond the space required to store N models (e.g. 42 GiB for each copy of T5-XXL), there is a substantial inference cost to running N distinct models, whether in parallel or in series. Prompt tuning provides a more efficient way to ensemble multiple adaptations of a pre-trained lan- guage model. By training N prompts on the same task, we create N separate “models” for a task, while still sharing the core language modeling pa- rameters throughout. Beyond drastically reducing storage costs, the prompt ensemble makes infer- ence more efficient. To process one example, rather than computing forward passes of N different mod- els, we can execute a single forward pass with a batch size of N, replicating the example across the batch and varying the prompt. These savings mirror those seen for multi-tasking in Figure 2. To demonstrate the viability of prompt ensem- bling, we train five prompts for each SuperGLUE task, using a single frozen T5-XXL model with our default hyperparameters. We use simple major- ity voting to compute predictions from the ensem- ble. Table 3 shows that across all tasks, the ensem- ble beats the single-prompt average and beats, or']```\n",
      "\n",
      "Respuesta:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "A continuación se proporciona una consulta del usuario y una lista de páginas extraídas de artículos de investigación separadas por triple comillas invertidas.  \n",
    "Tu tarea es extraer las piezas clave de información de esa lista en función de la consulta del usuario y redactarla como una respuesta completa en español. \n",
    "\n",
    "Consulta del usuario: ```{query}```\n",
    "Lista de paginas extraidas: ```{top_results['page_chunks'].to_list()}```\n",
    "\n",
    "Respuesta:\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt tuning es una técnica para adaptar modelos de lenguaje pre-entrenados a tareas específicas. A diferencia del ajuste de modelo, que requiere hacer una copia específica de todo el modelo pre-entrenado para cada tarea, el ajuste de prompt solo requiere almacenar un pequeño prompt específico de tarea para cada tarea, lo que permite la inferencia de tareas mixtas utilizando el modelo pre-entrenado original. Cada copia del modelo ajustado requiere 11 mil millones de parámetros, mientras que los prompts ajustados solo requieren 20,480 parámetros por tarea. Varios esfuerzos para automatizar el diseño de prompts han sido propuestos recientemente. Prompt tuning es una técnica más eficiente para ensembles de adaptaciones de modelos de lenguaje pre-entrenados.\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(prompt, chat_model)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def query_search(query, count=10):\n",
    "    results = search_client.search(search_text=query, top=count, include_total_count=True)\n",
    "    page_chunks = []\n",
    "    for result in results:\n",
    "        page_chunks.append(result['page_text'])\n",
    "        \n",
    "    #Create an embedding vector for each chunk that will capture the semantic meaning and overall topic of that chunk\n",
    "    embed_df['embedding'] = embed_df[\"page_chunks\"].apply(lambda page_text : get_embedding(page_text, engine = embedding_model))\n",
    "\n",
    "    query_embedding = get_embedding(query, engine=embedding_model)\n",
    "    embed_df[\"similarities\"] = embed_df['embedding'].apply(lambda page_embedding: cosine_similarity(page_embedding, query_embedding))\n",
    "\n",
    "    top_results = (\n",
    "        embed_df.sort_values(\"similarities\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "        .head(3)\n",
    "    )\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    A continuación se proporciona una consulta del usuario y una lista de páginas extraídas de artículos de investigación separadas por triple comillas invertidas.  \n",
    "Tu tarea es extraer las piezas clave de información de esa lista en función de la consulta del usuario y redactarla como una respuesta completa en español. \n",
    "\n",
    "Consulta del usuario: ```{query}```\n",
    "Lista de paginas extraidas: ```{top_results['page_chunks'].to_list()}```\n",
    "\n",
    "Respuesta:\n",
    "\"\"\"\n",
    "    \n",
    "    response = get_completion(prompt, chat_model)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La técnica de \"prompt tuning\" consiste en agregar información adicional para que el modelo se condicione durante la generación de una secuencia de tokens que representan una etiqueta de clase. A diferencia del ajuste de modelo, que requiere hacer una copia específica de todo el modelo pre-entrenado para cada tarea, el \"prompt tuning\" solo requiere almacenar un pequeño prompt específico de la tarea para cada tarea, lo que permite la inferencia de tareas mixtas utilizando el modelo pre-entrenado original. Además, el \"prompt tuning\" solo requiere un número reducido de parámetros por tarea, lo que reduce significativamente los costos de almacenamiento y procesamiento. Varios esfuerzos para automatizar el diseño de prompts han sido propuestos recientemente. En general, el \"prompt tuning\" es una técnica competitiva para adaptar modelos de lenguaje pre-entrenados a tareas específicas.\n"
     ]
    }
   ],
   "source": [
    "answer = query_search(\"How does prompt tuning work?\", 5)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Un sistema de recuperación denso completamente sin entrenamiento es un sistema que puede transferir el conocimiento de una tarea a otra sin entrenamiento adicional. En un estudio, se investigó la transferencia de dominio cero en dos tareas: detección de paráfrasis y respuesta a preguntas. Los resultados mostraron que la sintonización de la sugerencia superó el ajuste del modelo en la mayoría de los conjuntos de datos fuera del dominio, lo que sugiere que la sintonización de la sugerencia puede mejorar la robustez a los cambios de dominio. Además, se demostró que el ensemblado de sugerencias es una forma más eficiente de ensemblar múltiples adaptaciones de un modelo de lenguaje pre-entrenado. Finalmente, se discutió la interpretabilidad de las sugerencias aprendidas y se encontró que las sugerencias aprendidas forman grupos semánticos coherentes.\n"
     ]
    }
   ],
   "source": [
    "answer = query_search(\"what is a fully zero-shot dense retrieval system?\", 10)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
